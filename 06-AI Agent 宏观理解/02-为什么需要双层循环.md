# 为什么需要双层循环

## 一、双层循环的职责分离

OpenCode 的双层循环设计是为了**职责分离和精准控制**：

### 外层循环 - "任务管理器"

负责**整体任务推进**：

- 每次循环：从消息历史提取最新对话 → 调用 LLM 决策 → 执行工具
- 作用：管理 Agent 的**宏观执行流程**，控制整个任务的多个步骤
- 退出条件：任务完成或遇到错误

**代码位置**：`packages/opencode/src/session/prompt.ts:257`

```typescript
export const loop = fn(Identifier.schema("session"), async (sessionID) => {
  const abort = start(sessionID)
  if (!abort) {
    return new Promise<MessageV2.WithParts>((resolve, reject) => {
      const callbacks = state()[sessionID].callbacks
      callbacks.push({ resolve, reject })
    })
  }

  using _ = defer(() => cancel(sessionID))

  let step = 0
  const session = await Session.get(sessionID)
  while (true) {
    SessionStatus.set(sessionID, { type: "busy" })
    log.info("loop", { step, sessionID })
    if (abort.aborted) break

    // 获取消息历史
    let msgs = await MessageV2.filterCompacted(MessageV2.stream(sessionID))

    // 获取最后一条用户消息和助手消息
    let lastUser: MessageV2.User | undefined
    let lastAssistant: MessageV2.Assistant | undefined
    let lastFinished: MessageV2.Assistant | undefined
    let tasks: (MessageV2.CompactionPart | MessageV2.SubtaskPart)[] = []
    for (let i = msgs.length - 1; i >= 0; i--) {
      const msg = msgs[i]
      if (!lastUser && msg.info.role === "user") lastUser = msg.info as MessageV2.User
      if (!lastAssistant && msg.info.role === "assistant") lastAssistant = msg.info as MessageV2.Assistant
      if (!lastFinished && msg.info.role === "assistant" && msg.info.finish)
        lastFinished = msg.info as MessageV2.Assistant
      if (lastUser && lastFinished) break
      const task = msg.parts.filter((part) => part.type === "compaction" || part.type === "subtask")
      if (task && !lastFinished) {
        tasks.push(...task)
      }
    }

    if (!lastUser) throw new Error("No user message found in stream. This should never happen.")
    if (
      lastAssistant?.finish &&
      !["tool-calls", "unknown"].includes(lastAssistant.finish) &&
      lastUser.id < lastAssistant.id
    ) {
      log.info("exiting loop", { sessionID })
      break
    }

    step++
    if (step === 1)
      ensureTitle({
        session,
        modelID: lastUser.model.modelID,
        providerID: lastUser.model.providerID,
        message: msgs.find((m) => m.info.role === "user")!,
        history: msgs,
      })

    const model = await Provider.getModel(lastUser.model.providerID, lastUser.model.modelID)
    const task = tasks.pop()

    // pending subtask
    if (task?.type === "subtask") {
      // 执行子任务（省略详细代码）
      continue
    }

    // pending compaction
    if (task?.type === "compaction") {
      const result = await SessionCompaction.process({
        messages: msgs,
        parentID: lastUser.id,
        abort,
        sessionID,
        auto: task.auto,
      })
      if (result === "stop") break
      continue
    }

    // context overflow, needs compaction
    if (
      lastFinished &&
      lastFinished.summary !== true &&
      (await SessionCompaction.isOverflow({ tokens: lastFinished.tokens, model }))
    ) {
      await SessionCompaction.create({
        sessionID,
        agent: lastUser.agent,
        model: lastUser.model,
        auto: true,
      })
      continue
    }

    // normal processing
    const agent = await Agent.get(lastUser.agent)
    const maxSteps = agent.steps ?? Infinity
    const isLastStep = step >= maxSteps
    msgs = insertReminders({
      messages: msgs,
      agent,
    })

    const processor = SessionProcessor.create({
      assistantMessage: (await Session.updateMessage({
        id: Identifier.ascending("message"),
        parentID: lastUser.id,
        role: "assistant",
        mode: agent.name,
        agent: agent.name,
        path: {
          cwd: Instance.directory,
          root: Instance.worktree,
        },
        cost: 0,
        tokens: {
          input: 0,
          output: 0,
          reasoning: 0,
          cache: { read: 0, write: 0 },
        },
        modelID: model.id,
        providerID: model.providerID,
        time: {
          created: Date.now(),
        },
        sessionID,
      })) as MessageV2.Assistant,
      sessionID: sessionID,
      model,
      abort,
    })
    const tools = await resolveTools({
      agent,
      session,
      model,
      tools: lastUser.tools,
      processor,
    })

    if (step === 1) {
      SessionSummary.summarize({
        sessionID: sessionID,
        messageID: lastUser.id,
      })
    }

    const sessionMessages = clone(msgs)

    await Plugin.trigger("experimental.chat.messages.transform", {}, { messages: sessionMessages })

    const result = await processor.process({
      user: lastUser,
      agent,
      abort,
      sessionID,
      system: [...(await SystemPrompt.environment()), ...(await SystemPrompt.custom())],
      messages: [
        ...MessageV2.toModelMessage(sessionMessages),
        ...(isLastStep
          ? [
              {
                role: "assistant" as const,
                content: MAX_STEPS,
              },
            ]
          : []),
      ],
      tools,
      model,
    })
    if (result === "stop") break
    if (result === "compact") {
      await SessionCompaction.create({
        sessionID,
        agent: lastUser.agent,
        model: lastUser.model,
        auto: true,
      })
    }
    continue
  }
})
```

### 内层循环 - "单步执行器"

负责**单次 LLM 调用**的流式处理：

- 每次循环：处理 LLM 返回的一个事件（推理文本、工具调用、工具结果等）
- 作用：管理**工具生命周期**（pending → running → completed）和错误重试
- **核心职责**：**所有工具调用都在内层循环中执行**
- 退出条件：本次 LLM 调用完成

**完整事件类型列表**：

内层循环处理以下 14 种事件类型：

- `reasoning-start`: 推理过程开始
- `reasoning-delta`: 推理内容增量
- `reasoning-end`: 推理过程结束
- `tool-input-start`: 工具输入参数开始
- `tool-input-delta`: 工具输入参数增量
- `tool-input-end`: 工具输入参数结束
- `tool-call`: 工具调用执行（包含 Doom Loop 检测）
- `tool-result`: 工具执行结果
- `tool-error`: 工具执行错误
- `start-step`: 步骤开始
- `finish-step`: 步骤完成
- `text-start`: 文本输出开始
- `text-delta`: 文本输出增量
- `text-end`: 文本输出结束
- `finish`: 流式输出结束

**代码位置**：`packages/opencode/src/session/processor.ts:45`

```typescript
async process(streamInput: LLM.StreamInput) {
  const toolcalls: Record<string, MessageV2.ToolPart> = {}
  let blocked = false
  let attempt = 0

  while (true) {
    try {
      let currentText: MessageV2.TextPart | undefined
      let reasoningMap: Record<string, MessageV2.ReasoningPart> = {}

      // 调用 LLM 流式接口
      const stream = await LLM.stream(streamInput)

      // 处理 LLM 的流式输出
      for await (const value of stream.fullStream) {
        input.abort.throwIfAborted()

        switch (value.type) {
          case "start":
            SessionStatus.set(input.sessionID, { type: "busy" })
            break

          case "reasoning-delta":
            if (value.id in reasoningMap) {
              const part = reasoningMap[value.id]
              part.text += value.text
              if (value.providerMetadata) part.metadata = value.providerMetadata
              if (part.text) await Session.updatePart({ part, delta: value.text })
            }
            break

          case "tool-input-start":
            // 工具调用开始
            const part = await Session.updatePart({
              id: toolcalls[value.id]?.id ?? Identifier.ascending("part"),
              messageID: input.assistantMessage.id,
              sessionID: input.assistantMessage.sessionID,
              type: "tool",
              tool: value.toolName,
              callID: value.id,
              state: {
                status: "pending",
                input: {},
                raw: "",
              },
            })
            toolcalls[value.id] = part as MessageV2.ToolPart
            break

          case "tool-call":
            // 执行工具（包含 Doom Loop 检测）
            const match = toolcalls[value.toolCallId]
            if (match) {
              const part = await Session.updatePart({
                ...match,
                tool: value.toolName,
                state: {
                  status: "running",
                  input: value.input,
                  time: {
                    start: Date.now(),
                  },
                },
                metadata: value.providerMetadata,
              })
              toolcalls[value.toolCallId] = part as MessageV2.ToolPart

              // Doom Loop 检测：如果连续3次调用同一个工具且参数相同，则触发警告
              const parts = await MessageV2.parts(input.assistantMessage.id)
              const lastThree = parts.slice(-3)

              if (
                lastThree.length === 3 &&
                lastThree.every(
                  (p) =>
                    p.type === "tool" &&
                    p.tool === value.toolName &&
                    p.state.status !== "pending" &&
                    JSON.stringify(p.state.input) === JSON.stringify(value.input),
                )
              ) {
                const agent = await Agent.get(input.assistantMessage.agent)
                await PermissionNext.ask({
                  permission: "doom_loop",
                  patterns: [value.toolName],
                  sessionID: input.assistantMessage.sessionID,
                  metadata: {
                    tool: value.toolName,
                    input: value.input,
                  },
                  always: [value.toolName],
                  ruleset: agent.permission,
                })
              }
            }
            break

          case "tool-result":
            // 工具执行结果
            const match = toolcalls[value.toolCallId]
            if (match) {
              await Session.updatePart({
                ...match,
                state: {
                  status: "completed",
                  input: value.input,
                  output: value.output.output,
                  title: value.output.title,
                  time: { start: match.state.time.start, end: Date.now() },
                },
              })
            }
            delete toolcalls[value.toolCallId]
            break

          case "finish":
            // 标记流式输出结束，准备退出内层循环
            break
        }
      }
    } catch (e: any) {
      log.error("process", {
        error: e,
        stack: JSON.stringify(e.stack),
      })
      const error = MessageV2.fromError(e, { providerID: input.model.providerID })
      const retry = SessionRetry.retryable(error)
      if (retry !== undefined) {
        attempt++
        const delay = SessionRetry.delay(attempt, error.name === "APIError" ? error : undefined)
        SessionStatus.set(input.sessionID, {
          type: "retry",
          attempt,
          message: retry,
          next: Date.now() + delay,
        })
        await SessionRetry.sleep(delay, input.abort).catch(() => {})
        continue
      }
      input.assistantMessage.error = error
      Bus.publish(Session.Event.Error, {
        sessionID: input.assistantMessage.sessionID,
        error: input.assistantMessage.error,
      })
    }

    // 返回是否继续循环
    if (needsCompaction) return "compact"
    if (blocked) return "stop"
    if (input.assistantMessage.error) return "stop"
    return "continue"
  }
}
```

## 二、为什么必须分层？

### 单层循环无法胜任的场景

```
用户："帮我修改三个文件的 bug"

外层循环 step 1: LLM 调用
  └─ 内层循环处理：
      ├─ 工具1: read(file1.ts)
      ├─ 工具2: edit(file1.ts, 修复bug)
      ├─ 工具3: read(file2.ts)
      └─ 工具4: edit(file2.ts, 修复bug)
  → LLM 完成，返回 "continue"

外层循环 step 2: LLM 调用
  └─ 内层循环处理：
      ├─ 工具5: read(file3.ts)
      └─ 工具6: edit(file3.ts, 修复bug)
  → LLM 完成，返回 "stop"（任务完成）
```

### 核心区别

| 维度         | 外层循环                               | 内层循环                         |
| ------------ | -------------------------------------- | -------------------------------- |
| **时间粒度** | 多轮对话（step 1, step 2, ...）        | 单轮对话内的多工具调用           |
| **决策单元** | 每次 LLM 调用是一个决策单元            | 每个 LLM 流式事件是一个执行单元  |
| **状态管理** | 管理整个会话的步骤进度                 | 管理单个工具的执行状态           |
| **工具调用** | 不直接执行工具，只决定是否调用 LLM     | **所有工具调用都在此层执行**     |
| **退出条件** | 任务完成/错误/达到最大步数             | 本次 LLM 调用完成                |
| **返回值**   | 接收内层的 "continue"/"stop"/"compact" | 返回 "continue"/"stop"/"compact" |
| **重试粒度** | 整个步骤失败时重试                     | 单个工具或 LLM 调用失败时重试    |

**返回值说明**：

- `"continue"`: 正常完成，继续下一轮循环
- `"stop"`: 任务完成或遇到错误，终止循环
- `"compact"`: Token 使用量超过阈值，触发会话压缩（外层会创建 compaction 任务）

### 实际问题场景

如果用单层循环，无法区分：

1. "多个工具调用属于同一个 LLM 响应" vs "新的一次 LLM 决策"
2. "工具调用失败是否需要重新调用 LLM 决策" vs "直接重试该工具"
3. 会导致状态混乱，无法准确跟踪执行流程

## 三、为什么选择循环形式？

循环之所以是处理 LLM 流式输出的最佳选择，主要基于以下几个核心原因：

### 1. 流式处理的本质特性

LLM 的输出是**流式**的，不是一次性返回：

- 数据陆续到达，无法预知何时结束
- 需要持续监听并处理每个事件
- `for await` + 循环是处理异步流的标准模式

```typescript
// 这是处理异步流的自然写法
for await (const value of stream.fullStream) {
  switch (value.type) {
    case "tool-call":
      executeTool() // 持续处理直到流结束
  }
}
```

### 2. 不确定性无法预判

LLM 的输出是**完全不确定的**：

- 可能返回 0 个、1 个、N 个工具调用
- 每个工具的参数、执行时间都不同
- 无法提前知道"调用列表"，只能"来了一个处理一个"

**如果不用循环，需要：**

- 递归：需要维护显式调用栈，容易栈溢出
- 事件驱动：需要构建完整的事件系统，增加复杂度

### 3. 重试机制的便利性

循环的 `continue` 让重试非常自然：

```typescript
while (true) {
  try {
    // 调用 LLM
  } catch (e) {
    if (retryable) {
      continue // 直接跳过当前迭代，重新开始
    }
    break
  }
}
```

**其他形式的问题：**

- 递归：重试次数受栈深度限制
- 事件驱动：需要额外状态机跟踪重试次数

### 4. 状态共享的自然性

循环中的局部变量天然支持跨事件共享：

```typescript
const toolcalls: Record<string, ToolPart> = {}
let attempt = 0

for await (const value of stream) {
  // 所有事件都能访问 toolcalls 和 attempt
}
```

**其他形式的问题：**

- 函数式：需要通过参数传递或闭包维护状态
- 事件驱动：需要额外的 store/context 管理状态

### 5. 控制流的简洁性

循环能清晰表达"重复直到满足条件"的逻辑：

```typescript
while (true) {
  // 外层：重复执行步骤
  if (taskComplete) break

  const result = await processor.process()
  if (result === "stop") break
}
```

**对比其他形式：**

- 递归：`process()` -> `process()` -> `process()`... 难以追踪执行流程
- 状态机：需要维护大量状态转换规则

### 6. 核心原因总结

| 维度         | 循环               | 递归          | 事件驱动        |
| ------------ | ------------------ | ------------- | --------------- |
| **流式处理** | ✅ 天然支持        | ❌ 需要改造   | ⚠️ 需要额外封装 |
| **不确定性** | ✅ 来了就处理      | ❌ 难以预测   | ⚠️ 需要状态机   |
| **重试机制** | ✅ `continue` 简洁 | ❌ 栈深度限制 | ⚠️ 需要额外状态 |
| **状态共享** | ✅ 局部变量        | ⚠️ 参数传递   | ❌ 需要 store   |
| **代码简洁** | ✅ 最直观          | ⚠️ 递归深度   | ❌ 样板代码多   |
| **内存占用** | ✅ 常量级          | ❌ 线性增长   | ✅ 常量级       |

**核心原因**：循环是处理**"持续接收不确定事件"**这一问题的**最自然、最简洁、最高效**的形式，这正好匹配 LLM 流式输出的特性。

## 四、双层循环的协同工作

### 完整执行流程图

```
用户输入 (User Message)
    ↓
创建 User Message，保存到数据库
    ↓
进入外层循环 (while true) ←──────────────┐
    ↓                                      │
获取 session 实例                          │
    ↓                                      │
提取消息历史和最后一条消息                  │
    ↓                                      │
处理待定任务（subtask/compaction）         │
    ↓                                      │
创建 SessionProcessor                      │
    ↓                                      │
解析可用的工具集合                          │
    ↓                                      │
进入内层循环 (while true)                  │
    ↓                                      │
调用 LLM.stream()                          │
    ↓                                      │
处理流式事件（主要事件类型）：            │
  1. reasoning-delta (推理过程)             │
  2. tool-input-start (工具调用开始)        │
  3. tool-call (执行工具) ←──────────┐     │
  4. tool-result (工具结果)          │     │
  5. text-delta (文本输出)           │     │
  6. finish (步骤完成)               │     │
    ↓                                │     │
工具执行：                            │     │
  - 从 Tool Registry 获取工具定义    │     │
  - 验证参数 (Zod schema)            │     │
  - 执行工具逻辑（含 Doom Loop 检测） │     │
  - 返回结果                          │     │
    ↓                                │     │
将工具结果保存到 Message Part         │     │
    ↓                                │     │
LLM 继续处理，生成下一步决策          │     │
    ↓                                │     │
内层循环继续，直到 LLM 完成           │     │
    ↓                                │     │
返回 "continue"、"stop" 或 "compact" ────┘     │
    ↓                                      │
外层循环继续或终止（返回结果） ─────────────┘
```

## 五、双层循环的优势

### 1. 职责清晰

- **外层**：关注"何时调用 LLM"、"任务是否完成"
- **内层**：关注"如何处理流式输出"、"工具状态如何管理"

### 2. 错误隔离

- 工具调用失败只在内层重试，不影响外层流程
- LLM 调用失败在外层重试，不会导致整个流程崩溃

### 3. 精准控制

- 可以在内层实现工具级别的重试、Doom Loop 检测
- 可以在外层实现任务级别的步骤限制、压缩策略

### 4. 可扩展性

- 内层可以轻松添加新的流式事件类型
- 外层可以轻松添加新的任务控制策略

## 通俗解释：像餐厅里的"领班"和"厨师"

如果说上面的解释太像教科书，那我们换个更生活化的场景——**餐厅后厨**。

### 单层循环的问题（只有厨师）

想象一个只有厨师没有领班的餐厅：

1. 厨师炒完一道菜（处理一个任务），然后跑出去看下一位客人点什么菜（获取下一个任务）。
2. 这时候有个客人突然喊："服务员，我要退菜！"（高优先级中断）。
3. **问题来了**：厨师正在炒菜（内层执行），他是听不见外面的喊声的，必须等这道菜做完、端出去，才能处理退菜请求。
4. **结果**：客人的体验很差，因为响应太慢，界面会"卡顿"。

### 双层循环的解法（领班 + 厨师）

现在我们引入"双层循环"架构：

- **外层循环 = 领班（调度器/Scheduler）**
  - 领班站在大堂，手里拿着**任务小票**。
  - 他的工作是不断巡视："现在有哪些菜要做？哪桌客人等得最久？有没有加急单？"
  - 他负责**统筹规划**，决定任务处理的**优先级**和**时机**。他能敏锐地捕捉到客人的任何新需求。
- **内层循环 = 厨师（执行器/WorkLoop）**
  - 厨师只专注于灶台，两耳不闻窗外事。
  - 领班塞给他一张单子（一个工作单元），他就按部就班地切菜、炒菜、装盘（执行具体的逻辑）。

### 为什么这样更妙？

#### 1. 随时被打断（时间切片与可中断性）

当客人喊"加急"时，**外层的领班**立刻能听见。领班可以走到灶台拍拍厨师的肩膀：

> "先停一下手头的活，这个新单子优先级最高，先做这个。"
> 这就是为什么复杂应用（如 React 页面）在渲染大量数据时，双层循环能保证它不会卡死，因为外层循环可以随时插入用户的高优先级操作（比如点击、输入），切走时间片。

#### 2. 批量处理与优化

领班很聪明，他可以把三张不同桌面的"宫保鸡丁"合并成一个批次，告诉厨师：

> "这三个订单都是宫保鸡丁，你一次炒三份，省得重复洗锅。"
> 这就是双层循环带来的**批处理优化**能力，减少不必要的重复计算。

### 总结

| 维度         | 单层循环（只有厨师）     | 双层循环（领班+厨师）              |
| :----------- | :----------------------- | :--------------------------------- |
| **角色**     | 混乱，既要做决策又要干活 | 分工明确，各司其职                 |
| **响应速度** | 慢，必须干完活才能接客   | 快，随时能响应新需求               |
| **灵活性**   | 差，任务必须排队         | 强，可插队、可合并                 |
| **核心区别** | 执行与获取混杂           | **调度（思考）与执行（行动）分离** |

- **外层循环**：管"战略"。解决"**做什么、什么时候做**"的问题。
- **内层循环**：管"战术"。解决"**怎么做**"的问题。
  有了双层循环，系统才拥有了"思考"和"行动"分离的能力，变得更加智能和灵活。

## 六、总结

OpenCode 的双层循环设计是一个典型的**分层架构**，通过两层循环实现了不同粒度的控制：

- **外层循环** = 任务级控制，对应 Agent 的"感知-决策-执行-反馈"循环
- **内层循环** = 执行级控制，对应单个 LLM 调用的流式处理

这种设计使得 OpenCode 能够：

1. 准确区分不同层级的执行单元
2. 精细化管理任务和工具的状态
3. 实现可靠的错误恢复和重试机制
4. 支持复杂的 Agent 行为（如多 Agent 协作、任务分解）

**核心原则**：每层循环只关注自己层级的问题，通过清晰的接口进行协作，这是软件工程中"单一职责原则"的完美体现。
